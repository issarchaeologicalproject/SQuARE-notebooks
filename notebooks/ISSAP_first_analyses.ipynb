{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "AdNEJj1U42fC",
        "1U6NmpF1x5rr",
        "XdK-Zy46zHEf",
        "Q4u3Zhfd0oK1",
        "5uFbM3pF18pZ",
        "1_YuFvTq2xam",
        "RY62y0LO3wRC",
        "nfss_TDd4dUp"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## ISSAP Notebook for Initial Square Analysis\n",
        "\n",
        "Use the file tray at left to upload the ISSAP json file.\n",
        "\n",
        "Then write the filename in the codeblock below where it says 'input_file:', including the .json extension.\n",
        "\n",
        "Beside each code block (or series of code blocks) there is a 'play' symbol. You click on those in turn, progressing from the top of this notebook to the end. If you click on where it says 'x cells hidden' rather than the play icon, the cells will expand revealing the code.\n",
        "\n",
        "The file tray at left (file folder icon) sometimes needs to be refreshed to see the latest files (with the tray open, the folder with a reload arrow icon)."
      ],
      "metadata": {
        "id": "NtpLTbyzxWy6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title File to process\n",
        "# @markdown Forms support many types of fields.\n",
        "\n",
        "input_file = 'ISSAP_project_save - 2023-08-12T161845.315.json'  # @param {type: \"string\"}\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "jwH1hfKlxnLQ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialization\n",
        "\n",
        "This section contains code that sets up our ability to run the R code in the Brainerd Robinson section. Run this only once per session. It might take a minute or three to execute; please be patient and wait for the cells to run before moving on."
      ],
      "metadata": {
        "id": "AdNEJj1U42fC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## this takes *forever* ONLY RUN THIS BLOCK AND THE NEXT ONE ONCE\n",
        "## ignore warnings\n",
        "## This is to enable R to be interpreted in the Brainerd Robinson section\n",
        "%load_ext rpy2.ipython"
      ],
      "metadata": {
        "id": "TGfbcFPfzrPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "install.packages('statnet')"
      ],
      "metadata": {
        "id": "GKMqQN59zyVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initial data extraction and reshaping\n",
        "\n",
        "This code block performs the initial data extraction. It also defines several functions we use or might want to use.\n",
        "\n",
        "Run when starting a new session or when you are about to start a second round of analysis on a second json file.\n",
        "\n"
      ],
      "metadata": {
        "id": "1U6NmpF1x5rr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "4w8GcqizxVna"
      },
      "outputs": [],
      "source": [
        "# Not all functions are currently used.\n",
        "\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "\n",
        "def preprocess_json_data(json_data):\n",
        "    # Use regular expression to find and remove subdictionaries containing 'DELETE'\n",
        "    pattern = r'{\\\\\"arti_id\\\\\":\\\\\"arti_\\d+\\\\\",\\\\\"name\\\\\":\\\\\"DELETE.+?}'\n",
        "    for key in json_data:\n",
        "        for item in json_data[key]:\n",
        "            json_str = json.dumps(item)\n",
        "            json_str = re.sub(pattern, '', json_str)\n",
        "            item.clear()\n",
        "            item.update(json.loads(json_str))\n",
        "    return json_data\n",
        "\n",
        "def extract_subkeys_to_files(json_data, output_folder):\n",
        "    pattern = r'^ctxt_(\\d+)$'\n",
        "\n",
        "    for key, value_list in json_data.items():\n",
        "        match = re.match(pattern, key)\n",
        "        if match:\n",
        "            value = value_list[0]\n",
        "            subkeys_data = {\n",
        "                \"filename\": value.get(\"filename\", None),\n",
        "                \"square\": value.get(\"square\", None),\n",
        "                \"module\": value.get(\"module\", None),\n",
        "                \"oribtal_seg\": value.get(\"oribtal_seg\", None),\n",
        "                \"agency\": value.get(\"agency\", None),\n",
        "                \"interp\": value.get(\"interp\", None),\n",
        "                \"artifacts\": value.get(\"artifacts\", None),\n",
        "                \"description\": value.get(\"description\", None),\n",
        "                \"categories\": value.get(\"categories\", None),\n",
        "            }\n",
        "\n",
        "            # Create a file with the key name and write the subkeys data into it\n",
        "            filename = os.path.join(output_folder, f\"{key}.json\")\n",
        "            with open(filename, \"w\") as file:\n",
        "                json.dump(subkeys_data, file, indent=2)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    file = input_file\n",
        "\n",
        "    with open(file, \"r\") as json_file:\n",
        "        json_data = json.load(json_file)\n",
        "\n",
        "    # Preprocess the JSON data to remove subdictionaries containing 'DELETE'\n",
        "    json_data = preprocess_json_data(json_data)\n",
        "\n",
        "    output_folder = \"output_folder\"\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "    extract_subkeys_to_files(json_data, output_folder)\n",
        "\n",
        "### Centroids\n",
        "\n",
        "import re\n",
        "import json\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import csv\n",
        "import community\n",
        "\n",
        "def calculate_centroid(polygon_points):\n",
        "    # Extract the coordinates from the string\n",
        "    coordinates = re.findall(r'(\\d+\\.\\d+),(\\d+\\.\\d+)', polygon_points)\n",
        "    num_points = len(coordinates)\n",
        "\n",
        "    # Initialize variables for centroid coordinates\n",
        "    centroid_x = 0\n",
        "    centroid_y = 0\n",
        "\n",
        "    # Calculate the sum of x and y coordinates\n",
        "    for point in coordinates:\n",
        "        x, y = map(float, point)\n",
        "        centroid_x += x\n",
        "        centroid_y += y\n",
        "\n",
        "    # Calculate the average of x and y coordinates\n",
        "    centroid_x /= num_points\n",
        "    centroid_y /= num_points\n",
        "\n",
        "    return centroid_x, centroid_y\n",
        "\n",
        "# or use shapely for centroids\n",
        "from shapely.geometry import Polygon\n",
        "\n",
        "def calculate_centroid_shapely_method(polygon_points):\n",
        "    # Extract the coordinates from the string\n",
        "    coordinates = re.findall(r'(\\d+\\.\\d+),(\\d+\\.\\d+)', polygon_points)\n",
        "\n",
        "    # Create a Shapely polygon object\n",
        "    polygon = Polygon(coordinates)\n",
        "\n",
        "   # Calculate the centroid using the Shapely library\n",
        "    centroid = polygon.centroid\n",
        "\n",
        "    # Return the centroid coordinates\n",
        "    return centroid.x, centroid.y\n",
        "\n",
        "# or use polygon area method\n",
        "def calculate_centroid_pam(polygon_points):\n",
        "    # Extract the coordinates from the string\n",
        "    coordinates = re.findall(r'(\\d+\\.\\d+),(\\d+\\.\\d+)', polygon_points)\n",
        "\n",
        "    # Convert the coordinates to floats\n",
        "    vertices = [(float(x), float(y)) for x, y in coordinates]\n",
        "\n",
        "    # Calculate the signed area and centroid coordinates\n",
        "    signed_area = 0.0\n",
        "    centroid_x = 0.0\n",
        "    centroid_y = 0.0\n",
        "\n",
        "    for i in range(len(vertices)):\n",
        "        x_i, y_i = vertices[i]\n",
        "        x_next, y_next = vertices[(i + 1) % len(vertices)]  # Wrap around to the first vertex\n",
        "\n",
        "        # Compute the cross product of consecutive vertices\n",
        "        cross_product = (x_i * y_next) - (x_next * y_i)\n",
        "\n",
        "        # Accumulate the signed area and centroid coordinates\n",
        "        signed_area += cross_product\n",
        "        centroid_x += (x_i + x_next) * cross_product\n",
        "        centroid_y += (y_i + y_next) * cross_product\n",
        "\n",
        "    # Divide by 6 times the signed area to get the centroid coordinates\n",
        "    signed_area *= 0.5\n",
        "    centroid_x /= (6.0 * signed_area)\n",
        "    centroid_y /= (6.0 * signed_area)\n",
        "\n",
        "    # Return the centroid coordinates\n",
        "    return centroid_x, centroid_y\n",
        "\n",
        "\n",
        "def create_centroid_network(centroid_list, distance_threshold):\n",
        "    # Create an empty graph\n",
        "    G = nx.Graph()\n",
        "\n",
        "    # Add nodes to the graph\n",
        "    for i, centroid in enumerate(centroid_list):\n",
        "        arti_id = centroid['name']\n",
        "        G.add_node(arti_id, centroid=centroid)\n",
        "\n",
        "    # Calculate distances and add edges between centroids\n",
        "    for i in range(len(centroid_list)):\n",
        "        for j in range(i+1, len(centroid_list)):\n",
        "            centroid1 = centroid_list[i]\n",
        "            centroid2 = centroid_list[j]\n",
        "            distance = ((centroid1['centroid'][0] - centroid2['centroid'][0])**2 + (centroid1['centroid'][1] - centroid2['centroid'][1])**2)**0.5\n",
        "            if distance <= distance_threshold:\n",
        "                arti_id1 = centroid1['name']\n",
        "                arti_id2 = centroid2['name']\n",
        "                G.add_edge(arti_id1, arti_id2, distance=distance)\n",
        "\n",
        "    return G\n",
        "\n",
        "def export_graph_as_csv(graph, filepath):\n",
        "    with open(filepath, 'w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(['Source', 'Target', 'Distance'])\n",
        "\n",
        "        for edge in graph.edges(data=True):\n",
        "            source = edge[0]\n",
        "            target = edge[1]\n",
        "            distance = edge[2]['distance']\n",
        "            writer.writerow([source, target, distance])\n",
        "\n",
        "    print(f\"Graph exported as CSV file: {filepath}\")\n",
        "\n",
        "import numpy as np\n",
        "from scipy.spatial import ConvexHull\n",
        "\n",
        "def calculate_centroid_ConvexHull_method(polygon_points):\n",
        "    # Extract the coordinates from the string\n",
        "    coordinates = re.findall(r'(\\d+\\.\\d+),(\\d+\\.\\d+)', polygon_points)\n",
        "\n",
        "    # Convert the coordinates to a NumPy array\n",
        "    points = np.array(coordinates, dtype=float)\n",
        "\n",
        "    # Calculate the convex hull\n",
        "    hull = ConvexHull(points)\n",
        "\n",
        "    # Get the vertices of the convex hull\n",
        "    hull_vertices = points[hull.vertices]\n",
        "\n",
        "    # Calculate the centroid of the convex hull\n",
        "    centroid = np.mean(hull_vertices, axis=0)\n",
        "\n",
        "    # Return the centroid coordinates\n",
        "    return centroid[0], centroid[1]\n",
        "\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_polygons_with_centroids(polygon_data):\n",
        "    # Remove backslashes from the polygon data\n",
        "    polygon_data = polygon_data.replace('\\\\', '')\n",
        "\n",
        "    # Extract polygon data using regular expressions\n",
        "    matches = re.findall(r'<polygon points=\"([^\"]+)\"', polygon_data)\n",
        "\n",
        "    # Create a figure and axes\n",
        "    fig, ax = plt.subplots()\n",
        "\n",
        "    for polygon_points in matches:\n",
        "        # Extract the coordinates from the string\n",
        "        coordinates = re.findall(r'(\\d+\\.\\d+),(\\d+\\.\\d+)', polygon_points)\n",
        "        x_coords, y_coords = zip(*[(float(x), float(y)) for x, y in coordinates])\n",
        "\n",
        "        # Calculate the centroid\n",
        "        centroid_x = sum(x_coords) / len(x_coords)\n",
        "        centroid_y = sum(y_coords) / len(y_coords)\n",
        "\n",
        "        # Plot the polygon\n",
        "        ax.plot(x_coords, y_coords, 'b-')\n",
        "\n",
        "        # Plot the centroid\n",
        "        ax.plot(centroid_x, centroid_y, 'ro')\n",
        "\n",
        "    # Set the plot title and labels\n",
        "    ax.set_title('Polygons with Centroids')\n",
        "    ax.set_xlabel('X')\n",
        "    ax.set_ylabel('Y')\n",
        "\n",
        "    # Show the plot\n",
        "    plt.show()\n",
        "\n",
        "## close any polygons\n",
        "\n",
        "import re\n",
        "\n",
        "def identify_unclosed_polygons(polygon_data):\n",
        "    # Remove backslashes from the polygon data\n",
        "    polygon_data = polygon_data.replace('\\\\', '')\n",
        "\n",
        "    # Extract polygon data using regular expressions\n",
        "    matches = re.findall(r'<polygon points=\"([^\"]+)\"', polygon_data)\n",
        "\n",
        "    unclosed_polygons = []\n",
        "\n",
        "    for i, polygon_points in enumerate(matches):\n",
        "        # Extract the coordinates from the string\n",
        "        coordinates = re.findall(r'(\\d+\\.\\d+),(\\d+\\.\\d+)', polygon_points)\n",
        "\n",
        "        # Check if the polygon is closed\n",
        "        if coordinates[0] != coordinates[-1]:\n",
        "            unclosed_polygons.append(i)\n",
        "\n",
        "    return unclosed_polygons\n",
        "\n",
        "import re\n",
        "\n",
        "def close_polygons(polygon_data):\n",
        "    # Remove backslashes from the polygon data\n",
        "    polygon_data = polygon_data.replace('\\\\', '')\n",
        "\n",
        "    # Extract polygon data using regular expressions\n",
        "    matches = re.findall(r'<polygon points=\"([^\"]+)\"', polygon_data)\n",
        "\n",
        "    closed_polygon_data = polygon_data\n",
        "\n",
        "    for polygon_points in matches:\n",
        "        # Extract the coordinates from the string\n",
        "        coordinates = re.findall(r'(\\d+\\.\\d+),(\\d+\\.\\d+)', polygon_points)\n",
        "\n",
        "        # Check if the polygon is closed\n",
        "        if coordinates[0] != coordinates[-1]:\n",
        "            # Add the missing coordinate to close the polygon\n",
        "            closed_polygon_points = polygon_points + f' {coordinates[0][0]},{coordinates[0][1]}'\n",
        "            # Replace the original polygon points with the closed polygon points\n",
        "            closed_polygon_data = closed_polygon_data.replace(polygon_points, closed_polygon_points)\n",
        "\n",
        "    return closed_polygon_data\n",
        "\n",
        "# Function to close polygons in a JSON file and save the result to a new file\n",
        "def process_json_file(file_path, output_folder):\n",
        "    with open(file_path, 'r') as file:\n",
        "        polygon_data = file.read()\n",
        "\n",
        "    # Close polygons\n",
        "    closed_polygon_data = close_polygons(polygon_data)\n",
        "\n",
        "    # Create the subfolder if it doesn't exist\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    # Save the closed polygon data to a new file in the subfolder\n",
        "    filename = os.path.basename(file_path)\n",
        "    new_file_path = os.path.join(output_folder, 'closed_' + filename)\n",
        "    with open(new_file_path, 'w') as file:\n",
        "        file.write(closed_polygon_data)\n",
        "\n",
        "   # print(\"Polygons closed and saved to:\", new_file_path)\n",
        "\n",
        "# Folder path containing JSON files\n",
        "folder_path = 'output_folder'\n",
        "# Subfolder to save the closed polygon data\n",
        "output_folder = 'closed_polygon_data'\n",
        "\n",
        "# Iterate over all JSON files in the folder\n",
        "for filename in os.listdir(folder_path):\n",
        "    if filename.endswith('.json'):\n",
        "        file_path = os.path.join(folder_path, filename)\n",
        "        process_json_file(file_path, output_folder)\n",
        "\n",
        "## image functions\n",
        "\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "# Function to extract image width and height in pixels\n",
        "# question is, which bit of metadata is the correct metadata\n",
        "def extract_image_dimensions(json_data):\n",
        "    results = []\n",
        "    for key in json_data:\n",
        "        if 'ctxt_' in key:\n",
        "            try:\n",
        "                # Get the values of 'Image Width' and 'Image Height' keys\n",
        "                image_width = json_data[key][0]['exifInfo']['Image Width']['value']\n",
        "                image_height = json_data[key][0]['exifInfo']['Image Height']['value']\n",
        "                image_width2 = json_data[key][0]['exifInfo']['ImageWidth']['value']\n",
        "                image_length = json_data[key][0]['exifInfo']['ImageLength']['value']\n",
        "                PixelXDimension = json_data[key][0]['exifInfo']['PixelXDimension']['value']\n",
        "                PixelYDimension = json_data[key][0]['exifInfo']['PixelYDimension']['value']\n",
        "\n",
        "                # Extract ctxt key number\n",
        "                ctxt_num = key.split(\"_\")[1]  # Assumes ctxt is of format ctxt_x\n",
        "\n",
        "                results.append((ctxt_num, image_width, image_height, image_width2, image_length, PixelXDimension, PixelYDimension))\n",
        "            except KeyError:\n",
        "                results.append((None, None, None))\n",
        "    return results\n",
        "\n",
        "output_folder = \"analyzed_folder\"\n",
        "if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "\n",
        "# Initialize an empty DataFrame\n",
        "imagedf = pd.DataFrame(columns=['ctxt', 'image_width', 'image_height', 'image_width2', 'image_length', 'PixelXDimension','PixelYDimension'])\n",
        "\n",
        "# Open the file and read the lines\n",
        "with open(input_file, 'r') as f:\n",
        "    # Read the file line by line\n",
        "    for line in f:\n",
        "        # Load JSON data from each line\n",
        "        data = json.loads(line)\n",
        "\n",
        "        # Extract ctxt number, image width and length\n",
        "        dimensions = extract_image_dimensions(data)\n",
        "\n",
        "        # Append to the DataFrame\n",
        "        for ctxt, width, height, width2, length, pixelxdimension, pixelydimension in dimensions:\n",
        "            imagedf = pd.concat([imagedf, pd.DataFrame({'ctxt': [ctxt], 'image_width': [width], 'image_height': [height], 'image_width2': [width2], 'image_length': [length], 'PixelXDimension': [pixelxdimension], 'PixelYDimension': [pixelydimension]})], ignore_index=True)\n",
        "            imagedf.to_csv(f'analyzed_folder/image_pixel_dimension_info.csv', index=False)\n",
        "\n",
        "## centroid functions for working with network graphs, should we do that\n",
        "#centroid functions\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "from scipy.spatial.distance import cdist\n",
        "from scipy.spatial import distance_matrix\n",
        "import pandas as pd\n",
        "\n",
        "def calculate_relative_neighborhood(centroids, threshold):\n",
        "    # Calculate the pairwise distances between centroids\n",
        "    distances = cdist(centroids, centroids, metric='minkowski', p=1)\n",
        "\n",
        "    # Create an empty graph\n",
        "    G = nx.Graph()\n",
        "\n",
        "    # Add nodes to the graph\n",
        "    num_centroids = len(centroids)\n",
        "    for i in range(num_centroids):\n",
        "        G.add_node(i, pos=(centroids[i][0], centroids[i][1]))\n",
        "\n",
        "    # Add edges to the graph based on the relative neighborhood criterion\n",
        "    for i in range(num_centroids):\n",
        "        for j in range(i + 1, num_centroids):\n",
        "            if distances[i, j] <= threshold:\n",
        "                G.add_edge(i, j)\n",
        "\n",
        "    return G\n",
        "\n",
        "## rng per JB http://proceedings.caaconference.org/files/2011/42_Jimenez-Badillo_CAA2011.pdf\n",
        "def calculate_relative_neighbourhood_jb(centroids, beta=1.0):\n",
        "    # Initialize an empty graph\n",
        "    graph = nx.Graph()\n",
        "\n",
        "    # Compute the distance matrix\n",
        "    dist_matrix = distance_matrix(centroids, centroids)\n",
        "\n",
        "    # Check all pairs of points\n",
        "    for i in range(len(centroids)):\n",
        "        for j in range(i + 1, len(centroids)):\n",
        "            # Get the distance between points i and j, adjusted by beta\n",
        "            d = beta * dist_matrix[i, j]\n",
        "\n",
        "            # Check if any other point lies within the region RRNG\n",
        "            inside_rrng = False\n",
        "            for k in range(len(centroids)):\n",
        "                if k != i and k != j and max(dist_matrix[i, k], dist_matrix[j, k]) < d:\n",
        "                    inside_rrng = True\n",
        "                    break\n",
        "\n",
        "            # If no point is inside the RRNG, add an edge between i and j\n",
        "            if not inside_rrng:\n",
        "                graph.add_edge(i, j)\n",
        "\n",
        "    return graph\n",
        "\n",
        "# Function to process a single JSON file and create the network, community DataFrame, and name DataFrame\n",
        "def process_json_file(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        polygon_data = file.read()\n",
        "\n",
        "    # Your existing code to extract arti_id, name, and polygon_data\n",
        "# Remove backslashes from the polygon data\n",
        "    polygon_data = polygon_data.replace('\\\\', '')\n",
        "\n",
        "# matches = re.findall(r'<polygon points=\"([^\"]+)\"', polygon_data)\n",
        "    matches = re.findall(r'\"arti_id\":\"([^\"]+)\".*?\"name\":\"([^\"]+)\".*?\"type\":\"([^\"]+)\".*?\"subtype\":\"([^\"]+)\".*?<polygon points=\"([^\"]+)\"', polygon_data)\n",
        "\n",
        "    # Calculate centroids for each polygon and store arti_id and name in centroid_list\n",
        "    centroid_list = []\n",
        "    for match in matches:\n",
        "        arti_id, name, atype, subtype, polygon_points = match\n",
        "        centroid = calculate_centroid(polygon_points) ## change this line to use the calculate_centroid_pam or calculate_centroid_ConvexHull_method functions if you prefer\n",
        "        centroid_list.append({'arti_id': arti_id, 'name': name, 'type': atype, 'subtype': subtype, 'centroid': centroid})\n",
        "\n",
        "\n",
        "# Calculate the number of pixels wide using the right-most and left-most centroids\n",
        "    centroids = [centroid['centroid'] for centroid in centroid_list]  # Add this line\n",
        "    x_coords = [centroid[0] for centroid in centroids]\n",
        "    #num_pixels_wide = max(x_coords) - min(x_coords)\n",
        "\n",
        "## trying to add the actual widths\n",
        "    distance_threshold_percentage = 20  # Set the desired percentage here (e.g., 20%)\n",
        "\n",
        "# Loop through your contexts (assuming they are numbered sequentially starting at 0)\n",
        "    for i in range(len(imagedf)):\n",
        "    # Read the image_width value from the imagedf DataFrame\n",
        "        num_pixels_wide = imagedf.loc[i, 'image_width']\n",
        "\n",
        "    # If the image width value is not None and is a number\n",
        "        if num_pixels_wide is not None and isinstance(num_pixels_wide, (int, float)):\n",
        "        # Set the distance threshold as a percentage of the number of pixels wide\n",
        "            distance_threshold = (distance_threshold_percentage / 100) * num_pixels_wide\n",
        "            #print(f\"threshold for ctxt-{i} is {distance_threshold}\") #debugging statement\n",
        "        else:\n",
        "        # Handle the case where the image width is not available\n",
        "            print(f\"Image width not available for ctxt_{i}\")\n",
        "            num_pixels_wide = max(x_coords) - min(x_coords)\n",
        "            distance_threshold = (distance_threshold_percentage / 100) * num_pixels_wide\n",
        "\n",
        "\n",
        "# Create relative neighborhood network based on centroids\n",
        "    relative_neighborhood_network = calculate_relative_neighborhood(centroids, distance_threshold)\n",
        "    rng_jb = calculate_relative_neighbourhood_jb(centroids, 1.1) #higher beta, less connected\n",
        "   # Debugging print statements\n",
        "#    print(\"Number of nodes in the relative_neighborhood_network:\", len(relative_neighborhood_network.nodes))\n",
        "#    print(\"Number of edges in the relative_neighborhood_network:\", len(relative_neighborhood_network.edges))\n",
        "\n",
        "# communities?\n",
        "    partition = nx.community.greedy_modularity_communities(relative_neighborhood_network, resolution=1)\n",
        "# Debugging print statements\n",
        "#    print(\"Number of communities detected:\", len(partition))\n",
        "\n",
        "# Add community information to node attributes\n",
        "    for community_idx, community_nodes in enumerate(partition):\n",
        "        for node in community_nodes:\n",
        "            arti_id = centroid_list[node]['arti_id']\n",
        "            name = centroid_list[node]['name']\n",
        "            relative_neighborhood_network.nodes[node]['community'] = community_idx\n",
        "            relative_neighborhood_network.nodes[node]['arti_id'] = arti_id\n",
        "            relative_neighborhood_network.nodes[node]['name'] = name\n",
        "            relative_neighborhood_network.nodes[node]['type'] = atype\n",
        "            relative_neighborhood_network.nodes[node]['subtype'] = subtype\n",
        "\n",
        "# Create a table for the partition and its communities\n",
        "    data = []\n",
        "    for community_idx, community_nodes in enumerate(partition):\n",
        "        community_info = {\n",
        "            'Community Index': community_idx,\n",
        "            'Nodes (Centroids)': [node for node in community_nodes],\n",
        "            'arti_id': [centroid_list[node]['arti_id'] for node in community_nodes],\n",
        "            'Name': [centroid_list[node]['name'] for node in community_nodes],\n",
        "            'Type': [centroid_list[node]['type'] for node in community_nodes],\n",
        "            'Subtype': [centroid_list[node]['subtype'] for node in community_nodes]\n",
        "        # Add more attributes as needed\n",
        "        }\n",
        "        data.append(community_info)\n",
        "\n",
        "\n",
        "# Convert the list of dictionaries to a pandas DataFrame\n",
        "    community_df = pd.DataFrame(data)\n",
        "\n",
        "# Function to combine 'Name', 'Type', and 'Subtype'\n",
        "    def combine_names(row):\n",
        "      return ' '.join([f\"{name} {typ} {sub},\" for name, typ, sub in zip(row['Name'], row['Type'], row['Subtype'])])\n",
        "\n",
        "# Apply the function to create a new column 'the_things'\n",
        "    community_df['the_things'] = community_df.apply(combine_names, axis=1)\n",
        "\n",
        "# Flatten the 'the_things' column for counting\n",
        "    flattened_names = [name for sublist in community_df['the_things'] for name in sublist.split(',')]\n",
        "\n",
        "# aug 16 creating output just for subtypes, per Walsh request\n",
        "    # Function to combine 'Subtype'\n",
        "    def combine_subtypes(row):\n",
        "      return ' '.join([f\"{sub},\" for sub in row['Subtype']])\n",
        "\n",
        "# Apply the function to create a new column 'the_subtypes'\n",
        "    community_df['the_subtypes'] = community_df.apply(combine_subtypes, axis=1)\n",
        "\n",
        "# Flatten the 'the_subtypes' column for counting\n",
        "    flattened_subtypes = [subtype for sublist in community_df['the_subtypes'] for subtype in sublist.split(',')]\n",
        "\n",
        "# Calculate the count of each unique element in the flattened list\n",
        "    subtype_counts = pd.Series(flattened_subtypes).value_counts().reset_index()\n",
        "    subtype_counts.columns = ['Subtype', 'Count']\n",
        "\n",
        "\n",
        "# Calculate the count of each unique element in the flattened list\n",
        "    name_counts = pd.Series(flattened_names).value_counts().reset_index()\n",
        "    name_counts.columns = ['Name', 'Count']\n",
        "    return relative_neighborhood_network, community_df, name_counts, subtype_counts, rng_jb,\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic Counts\n",
        "\n",
        "The following code will create a subfolder within the 'analyzed_folder' with artefact counts by name, type, and subtype, including a summary table that can be further investigated.\n",
        "\n",
        "+ Artefacts by ctxt are in analyzed_folder/artefact_data\n",
        "+ Summary counts outputted to analyzed_folder/artefact_data/summary_file.csv\n",
        "\n",
        "The summary csv can be filtered by context, name, type, subtype, or a pivot table created as appropriate."
      ],
      "metadata": {
        "id": "XdK-Zy46zHEf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import json\n",
        "import re\n",
        "import os\n",
        "\n",
        "def process_json_file_for_artefacts(file_path, output_file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        polygon_data = file.read()\n",
        "\n",
        "    # Remove backslashes from the polygon data\n",
        "    polygon_data = polygon_data.replace('\\\\', '')\n",
        "    matches = re.findall(r'\"arti_id\":\"([^\"]+)\".*?\"name\":\"([^\"]+)\".*?\"type\":\"([^\"]+)\".*?\"subtype\":\"([^\"]+)\".*?\"material\":\"([^\"]+)\".*?\"colour\":\"([^\"]+)\".*?\"fixed\":\"([^\"]+)\".*?\"persistence\":\"([^\"]+)\".*?\"orientation\":\"([^\"]+)\".*?\"artiNotes\":\"([^\"]+)\".*?<polygon points=\"([^\"]+)\"', polygon_data)\n",
        "\n",
        "    artefact_list = []\n",
        "    for match in matches:\n",
        "        arti_id, name, atype, subtype, material, colour, fixed, persistence, orientation, artiNotes, polygon_points = match\n",
        "        artefact_list.append([arti_id, name, atype, subtype, material, colour, fixed, persistence, orientation, artiNotes, polygon_points])\n",
        "\n",
        "    # Writing to a csv file\n",
        "    with open(output_file_path, 'w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow([\"Arti_id\", \"Name\", \"Type\", \"Subtype\", \"Material\", \"Colour\", \"Fixed\", \"Persistence\", \"Orientation\", \"artiNotes\",\"polygon_data\"])\n",
        "        writer.writerows(artefact_list)\n",
        "\n",
        "def process_all_files_in_subfolder(input_folder, output_folder):\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    for root, dirs, files in os.walk(input_folder):\n",
        "        for filename in files:\n",
        "            if filename.endswith('.json'):\n",
        "                file_path = os.path.join(root, filename)\n",
        "                output_file_path = os.path.join(output_folder, f\"{filename}_artefacts.csv\")\n",
        "                process_json_file_for_artefacts(file_path, output_file_path)\n",
        "\n",
        "input_folder = \"closed_polygon_data\"\n",
        "output_folder = \"analyzed_folder/artefact_data\"\n",
        "process_all_files_in_subfolder(input_folder, output_folder)\n",
        "\n",
        "import csv\n",
        "import os\n",
        "from collections import defaultdict\n",
        "\n",
        "def create_summary_csv(output_folder, summary_file_path):\n",
        "    summary_data = defaultdict(lambda: defaultdict(lambda: defaultdict(int)))\n",
        "\n",
        "    for root, _, files in os.walk(output_folder):\n",
        "        for filename in files:\n",
        "            if filename.endswith('_artefacts.csv'):\n",
        "                file_path = os.path.join(root, filename)\n",
        "                with open(file_path, 'r', newline='') as csvfile:\n",
        "                    reader = csv.DictReader(csvfile)\n",
        "                    for row in reader:\n",
        "                        artiId = row ['Arti_id']\n",
        "                        name = row['Name']\n",
        "                        atype = row['Type']\n",
        "                        subtype = row['Subtype']\n",
        "                        material = row['Material']\n",
        "                        colour = row['Colour']\n",
        "                        fixed = row['Fixed']\n",
        "                        persistence = row['Persistence']\n",
        "                        orientation = row['Orientation']\n",
        "                        artiNotes = row['artiNotes']\n",
        "                        #summary_data[filename]['Arti_id'][artiId] += 1\n",
        "                        summary_data[filename]['Name'][name] += 1\n",
        "                        summary_data[filename]['Type'][atype] += 1\n",
        "                        summary_data[filename]['Subtype'][subtype] += 1\n",
        "                        summary_data[filename]['Material'][material] += 1\n",
        "                        summary_data[filename]['Colour'][colour] += 1\n",
        "                        summary_data[filename]['Fixed'][fixed] += 1\n",
        "                        summary_data[filename]['Persistence'][persistence] += 1\n",
        "                        summary_data[filename]['Orientation'][orientation] += 1\n",
        "                        summary_data[filename]['artiNotes'][artiNotes] += 1\n",
        "\n",
        "    # Writing the summary data to a CSV file\n",
        "    with open(summary_file_path, 'w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(['File', 'Category', 'Item', 'Count'])\n",
        "        for filename, categories in summary_data.items():\n",
        "            for category, items in categories.items():\n",
        "                for item, count in items.items():\n",
        "                    writer.writerow([filename, category, item, count])\n",
        "\n",
        "summary_file_path = \"analyzed_folder/artefact_data/summary_file.csv\"\n",
        "create_summary_csv(output_folder, summary_file_path)\n",
        "print(\"Artefacts by ctxt are in analyzed_folder/artefact_data.\")\n",
        "print(\"summary counts outputted to analyzed_folder/artefact_data/summary_file.csv\")\n",
        "print(\"The summary csv can be filtered by context, name, type, subtype, material, colour, fixed, persistence, orientation, artiNotes, or a pivot table created as appropriate.\")"
      ],
      "metadata": {
        "id": "U8j85KGkzMTf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Movement\n",
        "\n",
        "The following section tracks the persistence of artefacts from one context to the next. Some of the reshaping of data that occurs is important for when we push the data to the graph database."
      ],
      "metadata": {
        "id": "Q4u3Zhfd0oK1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import csv\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "def calculate_distance(point1, point2):\n",
        "    return np.sqrt((point1[0] - point2[0])**2 + (point1[1] - point2[1])**2)\n",
        "\n",
        "def preprocess_files_for_centroid_comparison_step(artefact_path):\n",
        "    artefacts_dfs = []\n",
        "    distance_threshold = 50  # to be adjusted per your dataset\n",
        "\n",
        "    for file in os.listdir(artefact_path):\n",
        "        if file.endswith('artefacts.csv'):\n",
        "            filepath = os.path.join(artefact_path, file)\n",
        "            context = file.split('_')[2].split('.')[0]\n",
        "\n",
        "            with open(filepath, 'r') as fp:\n",
        "                csv_reader = csv.reader(fp)\n",
        "                header = next(csv_reader)\n",
        "                rows_list = []\n",
        "                for row in csv_reader:\n",
        "                    # Same as before...\n",
        "                    arti_id, name, atype, subtype, material, colour, fixed, persistence, orientation, artiNotes, polygon_points = row\n",
        "                    polygon_points = polygon_points.replace('\"', '')\n",
        "                    centroid = calculate_centroid(polygon_points)\n",
        "                    centroid_string = '(' + ', '.join(str(num) for num in centroid) + ')'\n",
        "                    rows_list.append({'arti_id': arti_id, 'Artefact': name + '-' + atype + '-' + subtype, 'centroid': centroid_string, 'orientation': orientation, 'colour': colour, 'fixed': fixed, 'persistence': persistence, 'artiNotes': artiNotes, 'context': context})\n",
        "\n",
        "            data = pd.DataFrame(rows_list)\n",
        "\n",
        "            artifacts_coordinates = {}\n",
        "            count_artifact = {}\n",
        "            for _, row in data.iterrows():\n",
        "                artifact = row['Artefact']\n",
        "                centroid = tuple(map(float, row['centroid'].strip('()').split(',')))\n",
        "\n",
        "                is_close_to_another = False\n",
        "                for artifact_comp, centroid_comp in artifacts_coordinates.items():\n",
        "                    distance = calculate_distance(centroid, centroid_comp)\n",
        "                    if distance < distance_threshold:\n",
        "                        is_close_to_another = True\n",
        "                        common_base_name = artifact_comp\n",
        "                        break\n",
        "\n",
        "                if is_close_to_another:\n",
        "                    row['Artefact'] = common_base_name + str(count_artifact[common_base_name])\n",
        "                    count_artifact[common_base_name] += 1\n",
        "                else:\n",
        "                    artifacts_coordinates[artifact] = centroid\n",
        "                    count_artifact[artifact] = 0\n",
        "\n",
        "            data['Artefact'] = data['Artefact'] + data.groupby('Artefact').cumcount().add(1).astype(str)\n",
        "            artefacts_dfs.append(data)\n",
        "\n",
        "    return pd.concat(artefacts_dfs, ignore_index=True)\n",
        "\n",
        "def replace_orientation_errors(df):\n",
        "    df['orientation'] = df['orientation'].replace('o', 0)\n",
        "    return df\n",
        "\n",
        "def preprocess_files_for_finding_uniques_step(artefact_path):\n",
        "    artefacts2_dfs = []\n",
        "    for file in os.listdir(artefact_path):\n",
        "        if file.endswith('json_artefacts.csv'):\n",
        "            filepath = os.path.join(artefact_path, file)\n",
        "            context = file.split('_')[2].split('.')[0] # extracts 'ctxt_01' portion from filename\n",
        "\n",
        "            with open(filepath, 'r') as fp:\n",
        "              csv_reader = csv.reader(fp)\n",
        "              header = next(csv_reader)  # Skip the header row\n",
        "              rows_list = []\n",
        "              for row in csv_reader:\n",
        "                  arti_id, name, atype, subtype, material, colour, fixed, persistence, orientation, artiNotes, polygon_points = row\n",
        "                  polygon_points = polygon_points.replace('\"', '')  # Remove quotes\n",
        "                  centroid = calculate_centroid(polygon_points)\n",
        "                  centroid_string = '(' + ', '.join(str(num) for num in centroid) + ')'\n",
        "                  rows_list.append({'Artefact': name + '-' + atype + '-' + subtype, 'centroid': centroid_string, 'orientation': orientation, 'colour': colour, 'fixed': fixed, 'persistence': persistence, 'artiNotes': artiNotes, 'context': context})\n",
        "\n",
        "            # convert list of dict to dataframe\n",
        "            # generate 'rows_list' and convert to DataFrame\n",
        "        data = pd.DataFrame(rows_list)\n",
        "        artefacts2_dfs.append(data)\n",
        "\n",
        "# First, concatenate all data\n",
        "    all_data = pd.concat(artefacts2_dfs)\n",
        "\n",
        "# Then, check for unique 'Artefact' values across all data\n",
        "    counts = all_data['Artefact'].value_counts()\n",
        "    unique_artefacts = counts[counts == 1].index\n",
        "    all_data = all_data[all_data['Artefact'].isin(unique_artefacts)]\n",
        "    return all_data\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ki8aFSuW01jS"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "artefact_path = \"analyzed_folder/artefact_data\"\n",
        "final_df = preprocess_files_for_centroid_comparison_step(artefact_path)\n",
        "final_df = replace_orientation_errors(final_df)\n",
        "final_df.to_csv(f'{artefact_path}' + '/processed_artefacts.csv', index=False)\n",
        "\n",
        "# find uniques\n",
        "uniques_df = preprocess_files_for_finding_uniques_step(artefact_path)\n",
        "uniques_df = replace_orientation_errors(uniques_df)\n",
        "uniques_df.to_csv(f'{artefact_path}' + '/uniques.csv', index=False)"
      ],
      "metadata": {
        "id": "iGvajLiG032J"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## creating output that can be used as input for the notebook for creating the\n",
        "## graph database\n",
        "import pandas as pd\n",
        "\n",
        "def create_artefact_df(final_df):\n",
        "    final_df['context'] = final_df['context'].astype(int)\n",
        "    final_df['composite_id'] = 'ctxt_' + final_df['context'].astype(str) + '_'+ final_df['arti_id']\n",
        "    # Combine 'artifact' descriptions with composite_ids as a tuple\n",
        "    final_df['artefact_with_id'] = list(zip(final_df['Artefact'], final_df['composite_id']))\n",
        "    # Group by 'context' and aggregate the descriptions with their composite_ids\n",
        "    new_df = final_df.groupby('context')['artefact_with_id'].apply(list).reset_index()\n",
        "    new_df.sort_values('context', ascending=True, inplace=True)\n",
        "    return new_df\n",
        "\n",
        "def compare_context_artefacts(comparing_objects_df, context_i, context_j):\n",
        "    # Get the artefacts for each context\n",
        "    artefacts_i = dict(comparing_objects_df.loc[comparing_objects_df['context'] == context_i, 'artefact_with_id'].values[0])\n",
        "    artefacts_j = dict(comparing_objects_df.loc[comparing_objects_df['context'] == context_j, 'artefact_with_id'].values[0])\n",
        "\n",
        "    # Lists to keep track of artefacts\n",
        "    only_in_i = []\n",
        "    only_in_j = []\n",
        "    in_both = []\n",
        "    shared_items = []\n",
        "\n",
        "    # Comparisons\n",
        "    for artefact, comp_id in artefacts_i.items():\n",
        "        if artefact not in artefacts_j:\n",
        "            only_in_i.append((artefact, comp_id))\n",
        "        else:\n",
        "            in_both.append((artefact, comp_id))\n",
        "            shared_items.append((artefact, comp_id, artefacts_j[artefact]))\n",
        "\n",
        "    for artefact, comp_id in artefacts_j.items():\n",
        "        if artefact not in artefacts_i:\n",
        "            only_in_j.append((artefact, comp_id))\n",
        "\n",
        "    return only_in_i, in_both, only_in_j, shared_items\n",
        "\n",
        "def iterate_context_pairs(comparing_objects_df):\n",
        "    shared_results = []\n",
        "    results = []\n",
        "\n",
        "    context_values = comparing_objects_df[\"context\"].unique()\n",
        "    context_values.sort()\n",
        "\n",
        "    for i in range(len(context_values) - 1):\n",
        "        context_i = context_values[i]\n",
        "        context_j = context_values[i + 1]\n",
        "        only_in_i, in_both, only_in_j, shared_items = compare_context_artefacts(comparing_objects_df, context_i, context_j)\n",
        "\n",
        "        results.append({\n",
        "            'Objects ONLY in context i': [art for art, _ in only_in_i],\n",
        "            'Objects in BOTH contexts': [art for art, _ in in_both],\n",
        "            'Objects ONLY in context j': [art for art, _ in only_in_j]\n",
        "        })\n",
        "\n",
        "        shared_results.extend(shared_items)\n",
        "\n",
        "    # DataFrames for both CSVs\n",
        "    result_df = pd.DataFrame(results)\n",
        "    shared_df = pd.DataFrame(shared_results, columns=['artefact', 'composite_id_i', 'composite_id_j'])\n",
        "\n",
        "    return result_df, shared_df\n",
        "\n",
        "# Assuming final_df is already defined in your Jupyter notebook...\n",
        "comparing_objects_df = create_artefact_df(final_df)\n",
        "results, shared_artifacts = iterate_context_pairs(comparing_objects_df)\n",
        "\n",
        "# Export the first DataFrame results to CSV\n",
        "results.to_csv('analyzed_folder/artefact_data/NEO4_comparing_contexts_artefacts.csv', index=False)\n",
        "\n",
        "# Export the shared artifacts to another CSV\n",
        "shared_artifacts.to_csv('analyzed_folder/artefact_data/NEO4_shared_artefacts_across_contexts.csv', index=False)"
      ],
      "metadata": {
        "id": "ydh3WarR1gpA"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Brainerd-Robinson Similarity Coefficient and Sampling Error Assessment\n",
        "\n",
        "\n",
        "We use the code developed by [Matt Peeples](https://github.com/mpeeples2008/Brainerd-Robinson-Similarity-Coefficient-and-Sampling-Error-Assessment) with the R kernel here.\n",
        "\n",
        "When the actual BR code runs, it will ask you if the data is in counts or percent; choose counts. Then use 1000 for the number of random runs in order to assess the probability of the similarity.\n",
        "\n",
        "YOU MUST EXPAND THE CODE BLOCK and scroll down to see/interact with the user prompt."
      ],
      "metadata": {
        "id": "5uFbM3pF18pZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "def clean_count_artefacts(csv_file):\n",
        "\n",
        "    # 1. Read a csv file into a dataframe\n",
        "    df = pd.read_csv(csv_file)\n",
        "\n",
        "    # 2. Remove any digits from strings in the column 'Artefact'\n",
        "    df['Artefact'] = df['Artefact'].apply(lambda x: re.sub(r'\\d+', '', x))\n",
        "\n",
        "    # 3. Group by values in the column 'context'\n",
        "    group_data = df.groupby(['context', 'Artefact'])\n",
        "\n",
        "    # 4. Show the count of each item in 'artefact' by 'context'. If there are duplicate items, sum their counts.\n",
        "    group_data_counts = group_data.size().reset_index(name='Counts')\n",
        "\n",
        "    # Transpose data so that context is the index, artefacts are the columns, fill N/A with 0\n",
        "    result_data = group_data_counts.pivot(index='context', columns='Artefact', values='Counts').fillna(0)\n",
        "\n",
        "    return result_data\n",
        "\n",
        "def clean_subtypes_artefacts(csv_file):\n",
        "    # 1. Read a csv file into a dataframe\n",
        "    df = pd.read_csv(csv_file)\n",
        "\n",
        "    # 2. Keep only the string after the second dash in the column 'Artefact'\n",
        "    df['Artefact'] = df['Artefact'].apply(lambda x: x.split('-')[-1])\n",
        "\n",
        "    # 2.1 Remove any digits from strings in the column 'Artefact'\n",
        "    df['Artefact'] = df['Artefact'].apply(lambda x: re.sub(r'\\d+', '', x))\n",
        "\n",
        "    # 3. Group by values in the column 'context'\n",
        "    group_data = df.groupby(['context', 'Artefact'])\n",
        "\n",
        "    # 4. Show the count of each item in 'artefact' by 'context'. If there are duplicate items, sum their counts.\n",
        "    group_data_counts = group_data.size().reset_index(name='Counts')\n",
        "\n",
        "    # Transpose data so that context is the index, artefacts are the columns, fill N/A with 0\n",
        "    result_data = group_data_counts.pivot(index='context', columns='Artefact', values='Counts').fillna(0)\n",
        "\n",
        "    return result_data"
      ],
      "metadata": {
        "id": "DomW0-OL2W8w"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BR_counts = clean_count_artefacts(\"analyzed_folder/artefact_data/processed_artefacts.csv\")\n",
        "BR_counts.to_csv('analyzed_folder/BR_counts.csv')\n",
        "\n",
        "BR_subtype_counts = clean_subtypes_artefacts(\"analyzed_folder/artefact_data/processed_artefacts.csv\")\n",
        "BR_subtype_counts.to_csv(\"analyzed_folder/BR_subtype_counts.csv\")"
      ],
      "metadata": {
        "id": "WbUc8Cyn2ZPx"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# script by Matt Peeples, Matthew.Peeples@asu.edu, http://www.public.asu.edu/~mpeeple/\n",
        "\n",
        "library(statnet) # initialize necessary library\n",
        "\n",
        "# Function for calculating Brainerd-Robinson (BR) coefficients\n",
        "BR <- function(x) {\n",
        "rd <- dim(x)[1]\n",
        "results <- matrix(0,rd,rd)\n",
        "for (s1 in 1:rd) {\n",
        "for (s2 in 1:rd) {\n",
        "x1Temp <- as.numeric(x[s1, ])\n",
        "x2Temp <- as.numeric(x[s2, ])\n",
        "br.temp <- 0\n",
        "results[s1,s2] <- 200 - (sum(abs(x1Temp - x2Temp)))}}\n",
        "row.names(results) <- row.names(x)\n",
        "colnames(results) <- row.names(x)\n",
        "return(results)}\n",
        "\n",
        "# Obtain input table\n",
        "br.tab1 <- read.table(file='analyzed_folder/BR_counts.csv', sep=',', header=T, row.names=1) # the name of each row (site name) should be the first column in the input table\n",
        "#br.tab1 <- read.table(file='analyzed_folder/BR_subtype_counts.csv', sep=',', header=T, row.names=1) # the name of each row (site name) should be the first column in the input table\n",
        "\n",
        "#br.tab1 <- read.table(file='sq3_and_sq5_for_BR.csv', sep=',', header=T, row.names=1) # the name of each row (site name) should be the first column in the input table\n",
        "\n",
        "br.tab1 <- na.omit(br.tab1)\n",
        "\n",
        "# Ask for user if data are counts or percents\n",
        "choose.per <- function(){readline(\"Are the type data percents or counts? 1=percents, 2=counts : \")}\n",
        "per <- as.integer(choose.per())\n",
        "\n",
        "# If user selects counts, convert data to percents and run simulation\n",
        "if (per == 2) {\n",
        "br.tab <- prop.table(as.matrix(br.tab1),1)*100\n",
        "br.dat <- BR(br.tab) # actual BR values\n",
        "\n",
        "# Calculate the proportions of each category in the original data table\n",
        "c.sum <- matrix(0,1,ncol(br.tab1))\n",
        "for (i in 1:ncol(br.tab1)) {\n",
        "c.temp <- sum(br.tab1[,i])\n",
        "c.sum[,i] <- c.temp}\n",
        "p.grp <- prop.table(c.sum)\n",
        "\n",
        "# Create random sample of a specified sample size\n",
        "MC <- function(x,s.size) {\n",
        "v3 <- matrix(0,ncol(x),1)\n",
        "rand.tab.2 <- matrix(0,ncol(x),1)\n",
        "v <- sample(ncol(x),s.size,prob=p.grp,replace=T)\n",
        "v2 <- as.matrix(table(v))\n",
        "for (j in 1:nrow(v2)) {\n",
        "v3.temp <- v2[j,]\n",
        "v3[j,1] <- v3.temp}\n",
        "rand.tab <- as.matrix(prop.table(v3))*100\n",
        "rand.tab.2[,1] <- rand.tab\n",
        "return(rand.tab.2)}\n",
        "\n",
        "r.sums <- as.matrix(rowSums(br.tab1)) # Calculate sample size by row\n",
        "\n",
        "# Initate random samples for all rows\n",
        "BR_rand <- function(x) {\n",
        "rand.test <- matrix(0,ncol(x),nrow(r.sums))\n",
        "for (i in 1:nrow(x)) {\n",
        "rand.test[,i] <- MC(br.tab1,r.sums[i,])}\n",
        "return(rand.test)}\n",
        "\n",
        "br.diff.out <- matrix(0,nrow(br.dat),ncol(br.dat)) # Initialize table\n",
        "\n",
        "# Ask for user how many random counts to calculate\n",
        "choose.per <- function(){readline(\"How many random runs (1000 = recommended): \")}\n",
        "randruns <- as.integer(choose.per())\n",
        "\n",
        "# Run MC simulation of BR values\n",
        "for (i in 1:randruns) {\n",
        "br.temp <- BR_rand(br.tab1)\n",
        "br.temp <- t(br.temp)\n",
        "br.test <- BR(br.temp)\n",
        "br.diff <- br.dat - br.test\n",
        "br.diff2 <- event2dichot(br.diff,method='absolute',thresh=0)\n",
        "br.diff.out <- br.diff.out + br.diff2}\n",
        "\n",
        "br.diff.out <- br.diff.out / randruns # Calculate probabilities based on random runs\n",
        "row.names(br.diff.out) <- row.names(br.tab1)\n",
        "colnames(br.diff.out) <- row.names(br.tab1)\n",
        "\n",
        "write.table(br.diff.out,file='analyzed_folder/BR_prob.csv',sep=',') # Write output\n",
        "\n",
        "} # close if statement for count data\n",
        "\n",
        "# Recalculate actual BR values and output to file\n",
        "br.tab <- prop.table(as.matrix(br.tab1),1)*100\n",
        "br.dat <- BR(br.tab)\n",
        "write.table(br.dat,file='analyzed_folder/BR_out.csv',sep=',') # Write output\n",
        "\n",
        "# end of script"
      ],
      "metadata": {
        "id": "eqgRjNmN2cho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# script by Matt Peeples, Matthew.Peeples@asu.edu, http://www.public.asu.edu/~mpeeple/\n",
        "\n",
        "library(statnet) # initialize necessary library\n",
        "\n",
        "# Function for calculating Brainerd-Robinson (BR) coefficients\n",
        "BR <- function(x) {\n",
        "rd <- dim(x)[1]\n",
        "results <- matrix(0,rd,rd)\n",
        "for (s1 in 1:rd) {\n",
        "for (s2 in 1:rd) {\n",
        "x1Temp <- as.numeric(x[s1, ])\n",
        "x2Temp <- as.numeric(x[s2, ])\n",
        "br.temp <- 0\n",
        "results[s1,s2] <- 200 - (sum(abs(x1Temp - x2Temp)))}}\n",
        "row.names(results) <- row.names(x)\n",
        "colnames(results) <- row.names(x)\n",
        "return(results)}\n",
        "\n",
        "# Obtain input table\n",
        "#br.tab1 <- read.table(file='analyzed_folder/BR_counts.csv', sep=',', header=T, row.names=1) # the name of each row (site name) should be the first column in the input table\n",
        "br.tab1 <- read.table(file='analyzed_folder/BR_subtype_counts.csv', sep=',', header=T, row.names=1) # the name of each row (site name) should be the first column in the input table\n",
        "\n",
        "#br.tab1 <- read.table(file='sq3_and_sq5_for_BR.csv', sep=',', header=T, row.names=1) # the name of each row (site name) should be the first column in the input table\n",
        "\n",
        "br.tab1 <- na.omit(br.tab1)\n",
        "\n",
        "# Ask for user if data are counts or percents\n",
        "choose.per <- function(){readline(\"Are the type data percents or counts? 1=percents, 2=counts : \")}\n",
        "per <- as.integer(choose.per())\n",
        "\n",
        "# If user selects counts, convert data to percents and run simulation\n",
        "if (per == 2) {\n",
        "br.tab <- prop.table(as.matrix(br.tab1),1)*100\n",
        "br.dat <- BR(br.tab) # actual BR values\n",
        "\n",
        "# Calculate the proportions of each category in the original data table\n",
        "c.sum <- matrix(0,1,ncol(br.tab1))\n",
        "for (i in 1:ncol(br.tab1)) {\n",
        "c.temp <- sum(br.tab1[,i])\n",
        "c.sum[,i] <- c.temp}\n",
        "p.grp <- prop.table(c.sum)\n",
        "\n",
        "# Create random sample of a specified sample size\n",
        "MC <- function(x,s.size) {\n",
        "v3 <- matrix(0,ncol(x),1)\n",
        "rand.tab.2 <- matrix(0,ncol(x),1)\n",
        "v <- sample(ncol(x),s.size,prob=p.grp,replace=T)\n",
        "v2 <- as.matrix(table(v))\n",
        "for (j in 1:nrow(v2)) {\n",
        "v3.temp <- v2[j,]\n",
        "v3[j,1] <- v3.temp}\n",
        "rand.tab <- as.matrix(prop.table(v3))*100\n",
        "rand.tab.2[,1] <- rand.tab\n",
        "return(rand.tab.2)}\n",
        "\n",
        "r.sums <- as.matrix(rowSums(br.tab1)) # Calculate sample size by row\n",
        "\n",
        "# Initate random samples for all rows\n",
        "BR_rand <- function(x) {\n",
        "rand.test <- matrix(0,ncol(x),nrow(r.sums))\n",
        "for (i in 1:nrow(x)) {\n",
        "rand.test[,i] <- MC(br.tab1,r.sums[i,])}\n",
        "return(rand.test)}\n",
        "\n",
        "br.diff.out <- matrix(0,nrow(br.dat),ncol(br.dat)) # Initialize table\n",
        "\n",
        "# Ask for user how many random counts to calculate\n",
        "choose.per <- function(){readline(\"How many random runs (1000 = recommended): \")}\n",
        "randruns <- as.integer(choose.per())\n",
        "\n",
        "# Run MC simulation of BR values\n",
        "for (i in 1:randruns) {\n",
        "br.temp <- BR_rand(br.tab1)\n",
        "br.temp <- t(br.temp)\n",
        "br.test <- BR(br.temp)\n",
        "br.diff <- br.dat - br.test\n",
        "br.diff2 <- event2dichot(br.diff,method='absolute',thresh=0)\n",
        "br.diff.out <- br.diff.out + br.diff2}\n",
        "\n",
        "br.diff.out <- br.diff.out / randruns # Calculate probabilities based on random runs\n",
        "row.names(br.diff.out) <- row.names(br.tab1)\n",
        "colnames(br.diff.out) <- row.names(br.tab1)\n",
        "\n",
        "write.table(br.diff.out,file='analyzed_folder/BR_subtypes_prob.csv',sep=',') # Write output\n",
        "\n",
        "} # close if statement for count data\n",
        "\n",
        "# Recalculate actual BR values and output to file\n",
        "br.tab <- prop.table(as.matrix(br.tab1),1)*100\n",
        "br.dat <- BR(br.tab)\n",
        "write.table(br.dat,file='analyzed_folder/BR_subtype_out.csv',sep=',') # Write output\n",
        "\n",
        "# end of script"
      ],
      "metadata": {
        "id": "GIK62gAQ2hPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initial Visualizations\n",
        "\n",
        "We produce a heatmap for contexts against contexts and their BR similarity coefficients.\n",
        "\n",
        "We produce voronoi diagram for artefacts by contexts.\n",
        "\n",
        "In the 'voronoi' code block, you may wish to adjust line 13 for the list of major types.\n",
        "\n",
        "Plots will be displayed below (click on '2 cells hidden'), and also written to the 'analyzed_folder'."
      ],
      "metadata": {
        "id": "1_YuFvTq2xam"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df = pd.read_csv(\"analyzed_folder/BR_out.csv\")  # loading the data\n",
        "sns.heatmap(df, cmap='coolwarm', linewidths=.5)\n",
        "plt.suptitle('BR Coefficient of Similarity Matrix, by Type', fontsize=16)\n",
        "plt.savefig('analyzed_folder/sq3_BR_by_type-300dpi.png', dpi=300,bbox_inches=\"tight\")\n",
        "plt.show()\n",
        "\n",
        "df2 = pd.read_csv(\"analyzed_folder/BR_subtype_out.csv\")  # loading the data\n",
        "sns.heatmap(df2, cmap='coolwarm', linewidths=.5)\n",
        "plt.suptitle('BR Coefficient of Similarity Matrix, by Subtype', fontsize=16)\n",
        "plt.savefig('analyzed_folder/sq_3BR_by_subtype-300dpi.png', dpi=300,bbox_inches=\"tight\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VEKWubXg2ppI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# voronoi by major subtypes, mosaic colouration\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from scipy.spatial import Voronoi, voronoi_plot_2d\n",
        "import seaborn as sns\n",
        "import matplotlib.patches as mpatches\n",
        "from shapely.geometry import Polygon\n",
        "from shapely.ops import unary_union, polygonize\n",
        "from scipy.spatial import Delaunay\n",
        "\n",
        "#allowed_subtypes = ['restraint', 'tool', 'container', 'writing', 'experiment'] #sq3\n",
        "allowed_subtypes = ['tool', 'container', 'writing', 'experiment'] #sq3 jan 10 with 'restraint' removed\n",
        "#allowed_subtypes = ['restraint', 'body maintenance', 'computing', 'power', 'container'] #sq5\n",
        "#allowed_subtypes = ['body maintenance', 'computing', 'power', 'container'] #sq5 Justin asks if 'restraint' removed, what does it look like\n",
        "\n",
        "def preprocess_data(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df['subtype'] = df['Artefact'].str.split('-').str[2].str.replace('\\d+', '', regex=True).str.lower()\n",
        "    return df[df['subtype'].isin(allowed_subtypes)]\n",
        "\n",
        "def plot(data: pd.DataFrame):\n",
        "    data = preprocess_data(data)\n",
        "\n",
        "    # Get unique subtypes and assign them a color\n",
        "    subtype_colors = {subtype: color for subtype, color\n",
        "                      in zip(data['subtype'].unique(), sns.color_palette('hsv', n_colors=data['subtype'].nunique()))}\n",
        "\n",
        "    def voronoi_finite_polygons_2d(vor, radius=None):\n",
        "        \"\"\"\n",
        "        Reconstruct infinite voronoi regions in a 2D diagram to finite\n",
        "        regions.\n",
        "        \"\"\"\n",
        "        if vor.points.shape[1] != 2:\n",
        "            raise ValueError(\"Requires 2D input\")\n",
        "\n",
        "        new_regions = []\n",
        "        new_vertices = vor.vertices.tolist()\n",
        "\n",
        "        center = vor.points.mean(axis=0)\n",
        "        if radius is None:\n",
        "            radius = vor.points.ptp().max()\n",
        "\n",
        "        # Construct a map containing all ridges for a given point\n",
        "        all_ridges = {}\n",
        "        for (p1, p2), (v1, v2) in zip(vor.ridge_points, vor.ridge_vertices):\n",
        "            all_ridges.setdefault(p1, []).append((p2, v1, v2))\n",
        "            all_ridges.setdefault(p2, []).append((p1, v1, v2))\n",
        "\n",
        "        # Reconstruct infinite regions\n",
        "        for p1, region in enumerate(vor.point_region):\n",
        "            vertices = vor.regions[region]\n",
        "\n",
        "            if all(v >= 0 for v in vertices):\n",
        "                # Finite region\n",
        "                new_regions.append(vertices)\n",
        "                continue\n",
        "\n",
        "            # Reconstruct a non-finite region\n",
        "            ridges = all_ridges[p1]\n",
        "            new_region = [v for v in vertices if v >= 0]\n",
        "\n",
        "            for p2, v1, v2 in ridges:\n",
        "                if v2 < 0:\n",
        "                    v1, v2 = v2, v1\n",
        "                if v1 >= 0:\n",
        "                    # Finite ridge: already in the region\n",
        "                    continue\n",
        "\n",
        "                # Compute the missing endpoint of an infinite ridge\n",
        "                t = vor.points[p2] - vor.points[p1]  # tangent\n",
        "                t /= np.linalg.norm(t)\n",
        "                n = np.array([-t[1], t[0]])  # normal\n",
        "\n",
        "                midpoint = vor.points[[p1, p2]].mean(axis=0)\n",
        "                direction = np.sign(np.dot(midpoint - center, n)) * n\n",
        "                far_point = vor.vertices[v2] + direction * radius\n",
        "\n",
        "                new_region.append(len(new_vertices))\n",
        "                new_vertices.append(far_point.tolist())\n",
        "\n",
        "            # Sort region counterclockwise.\n",
        "            vs = np.asarray([new_vertices[v] for v in new_region])\n",
        "            c = vs.mean(axis=0)\n",
        "            angles = np.arctan2(vs[:, 1] - c[1], vs[:, 0] - c[0])\n",
        "            new_region = np.array(new_region)[np.argsort(angles)]\n",
        "\n",
        "            # Finish\n",
        "            new_regions.append(new_region.tolist())\n",
        "\n",
        "        return new_regions, np.asarray(new_vertices)\n",
        "\n",
        "    fig, axs = plt.subplots(nrows=6, ncols=10, figsize=(20, 12))\n",
        "    for i, ax in enumerate(axs.flatten()):\n",
        "        if i < len(data['context'].unique()):\n",
        "            context = data[data['context'] == i]\n",
        "            points = context['centroid'].str.extract(r'\\((.*),(.*)\\)', expand=True).astype(float).values\n",
        "            vor = Voronoi(points)\n",
        "            regions, vertices = voronoi_finite_polygons_2d(vor)\n",
        "\n",
        "            centroids = context[['centroid', 'subtype']]\n",
        "            for region_index, region in enumerate(regions):\n",
        "                polygon = vertices[region]\n",
        "                # Fill the region with color\n",
        "                color = subtype_colors[centroids.iloc[region_index]['subtype']]\n",
        "                ax.fill(*zip(*polygon), color=color, alpha=0.4)\n",
        "\n",
        "            voronoi_plot_2d(vor, ax=ax, show_vertices=False, line_colors='orange',\n",
        "                            line_width=1, line_alpha=0.5, point_size=2)\n",
        "\n",
        "            ax.set_title(f'Context {i}')\n",
        "            ax.set_xlim(vor.min_bound[0] - 0.1, vor.max_bound[0] + 0.1)\n",
        "            ax.set_ylim(vor.min_bound[1] - 0.1, vor.max_bound[1] + 0.1)\n",
        "            ax.set_xticks([])\n",
        "            ax.set_yticks([])\n",
        "        else:\n",
        "            ax.axis('off')\n",
        "\n",
        "    # Create legend patches\n",
        "    legend_patches = [mpatches.Patch(color=color, label=subtype) for subtype, color in subtype_colors.items()]\n",
        "    # Add the legend to the plot\n",
        "    plt.legend(handles=legend_patches, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "    plt.suptitle('Voronoi Map of Artefact Centroids by Context', fontsize=16)\n",
        "    plt.tight_layout(rect=[0, 0, 0.85, 0.95])  # Make space for legend\n",
        "    plt.savefig('analyzed_folder/voronoi-300dpi.png', dpi=300,bbox_inches=\"tight\")\n",
        "    return plt\n",
        "\n",
        "data = pd.read_csv(\"analyzed_folder/artefact_data/processed_artefacts.csv\")\n",
        "\n",
        "chart = plot(data)\n",
        "chart.show()"
      ],
      "metadata": {
        "id": "b4VUqicn2wAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Export\n",
        "The next codeblock exports everything in the 'analyzed_folder'. It zips the folder up, and then downloads automatically. Be patient."
      ],
      "metadata": {
        "id": "RY62y0LO3wRC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r ISSAP-results.zip analyzed_folder/\n",
        "from google.colab import files\n",
        "files.download(\"ISSAP-results.zip\")"
      ],
      "metadata": {
        "id": "OwqWDc_Q3xir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clean UP\n",
        "\n",
        "If you are going to analyze another json file in the current session, run the code below to clean up the workspace. YOU DO NOT NEED to run the initialization step, but you SHOULD change the input_file value to the name of your new json file, and then start by running from the Initial Data Extraction step downwards."
      ],
      "metadata": {
        "id": "nfss_TDd4dUp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! rm -r output_folder\n",
        "! rm -r analyzed_folder\n",
        "! rm -r closed_polygon_data"
      ],
      "metadata": {
        "id": "VNlTzwDo4c2o"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}